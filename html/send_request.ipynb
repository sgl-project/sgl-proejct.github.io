{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch a server\n",
    "\n",
    "This code uses `subprocess.Popen` to start an SGLang server process, equivalent to executing \n",
    "\n",
    "```bash\n",
    "python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "--port 30000 --host 0.0.0.0 --log-level warning\n",
    "```\n",
    "in your command line and wait for the server to be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is ready. Proceeding with the next steps.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "\n",
    "server_process = subprocess.Popen(\n",
    "    [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"sglang.launch_server\",\n",
    "        \"--model-path\",\n",
    "        \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        \"--port\",\n",
    "        \"30000\",\n",
    "        \"--host\",\n",
    "        \"0.0.0.0\",\n",
    "        \"--log-level\",\n",
    "        \"error\",\n",
    "    ],\n",
    "    text=True,\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            \"http://localhost:30000/v1/models\",\n",
    "            headers={\"Authorization\": \"Bearer None\"},\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            break\n",
    "    except requests.exceptions.RequestException:\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"Server is ready. Proceeding with the next steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a Request\n",
    "\n",
    "Once the server is running, you can send test requests using curl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"796551b41bb44407aba536666f97a8f9\",\"object\":\"chat.completion\",\"created\":1729785119,\"model\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"LLM stands for Large Language Model. It's a type of artificial intelligence (AI) designed to process and generate human-like language. LLMs are trained on vast amounts of text data, which allows them to understand the structure, syntax, and semantics of language.\\n\\nA Large Language Model works by taking in a prompt or input, analyzing it, and then generating a response based on the patterns and relationships it has learned from the training data. This can include tasks such as:\\n\\n1. Language translation\\n2. Text summarization\\n3. Sentiment analysis\\n4. Question answering\\n5. Text generation (e.g., writing stories, poems, or even entire articles)\\n\\nThe key characteristics of LLMs include:\\n\\n1. **Scale**: They are trained on massive amounts of text data, often in the order of hundreds of billions of parameters.\\n2. **Depth**: They use complex neural network architectures to process and analyze language.\\n3. **Self-supervised learning**: They learn from the data itself, without explicit supervision or labeling.\\n4. **Contextual understanding**: They can understand the context and nuances of language, including idioms, colloquialisms, and figurative language.\\n\\nLLMs have many applications, including:\\n\\n1. Virtual assistants (like myself)\\n2. Chatbots\\n3. Language translation tools\\n4. Content generation platforms\\n5. Research and analysis tools\\n\\nThe most well-known LLMs are:\\n\\n1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google in 2018.\\n2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook AI in 2019.\\n3. **LLaMA**: Developed by Meta AI in 2022.\\n\\nThese models have achieved impressive results in various natural language processing tasks and have democratized access to AI-powered language capabilities.\\n\\nDo you have any specific questions about LLMs or their applications?\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":128009}],\"usage\":{\"prompt_tokens\":47,\"total_tokens\":440,\"completion_tokens\":393,\"prompt_tokens_details\":null}}"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:30000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer None\" \\\n",
    "  -d '{\"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is a LLM?\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Compatible API\n",
    "\n",
    "SGLang supports OpenAI-compatible APIs. Here are Python examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='5e7e702c54994d9b9258cb0678878e27', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are 3 countries and their capitals:\\n\\n1. **Country:** Japan\\n**Capital:** Tokyo\\n\\n2. **Country:** Australia\\n**Capital:** Canberra\\n\\n3. **Country:** Brazil\\n**Capital:** Bras√≠lia', refusal=None, role='assistant', function_call=None, tool_calls=None), matched_stop=128009)], created=1729785135, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=46, prompt_tokens=49, total_tokens=95, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Always assign an api_key, even if not specified during server initialization.\n",
    "# Setting an API key during server initialization is strongly recommended.\n",
    "\n",
    "client = openai.Client(\n",
    "    base_url=\"http://127.0.0.1:30000/v1\", api_key=\"None\"\n",
    ")\n",
    "\n",
    "# Chat completion example\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Embedding Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding server is ready. Proceeding with the next steps.\n"
     ]
    }
   ],
   "source": [
    "server_process.terminate()\n",
    "\n",
    "# Start a new server process for embedding models\n",
    "# Equivalent to running this in the shell:\n",
    "# python -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-7B-instruct --port 30010 --host 0.0.0.0 --is-embedding --log-level error\n",
    "embedding_process = subprocess.Popen(\n",
    "    [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"sglang.launch_server\",\n",
    "        \"--model-path\",\n",
    "        \"Alibaba-NLP/gte-Qwen2-7B-instruct\",\n",
    "        \"--port\",\n",
    "        \"30010\",\n",
    "        \"--host\",\n",
    "        \"0.0.0.0\",\n",
    "        \"--is-embedding\",\n",
    "        \"--log-level\",\n",
    "        \"error\",\n",
    "    ],\n",
    "    text=True,\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            \"http://localhost:30010/v1/models\",\n",
    "            headers={\"Authorization\": \"Bearer None\"},\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            break\n",
    "    except requests.exceptions.RequestException:\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"Embedding server is ready. Proceeding with the next steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0083160400390625, 0.0006804466247558594, -0.00809478759765625, -0.0006995201110839844, 0.0143890380859375, -0.0090179443359375, 0.01238250732421875, 0.00209808349609375, 0.0062103271484375, -0.003047943115234375]\n"
     ]
    }
   ],
   "source": [
    "# Get the first 10 elements of the embedding\n",
    "\n",
    "! curl -s http://localhost:30010/v1/embeddings \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer None\" \\\n",
    "  -d '{\"model\": \"Alibaba-NLP/gte-Qwen2-7B-instruct\", \"input\": \"Once upon a time\"}' \\\n",
    "  | python3 -c \"import sys, json; print(json.load(sys.stdin)['data'][0]['embedding'][:10])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00603485107421875, -0.0190582275390625, -0.01273345947265625, 0.01552581787109375, 0.0066680908203125, -0.0135955810546875, 0.01131439208984375, 0.0013713836669921875, -0.0089874267578125, 0.021759033203125]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.Client(\n",
    "    base_url=\"http://127.0.0.1:30010/v1\", api_key=\"None\"\n",
    ")\n",
    "\n",
    "# Text embedding example\n",
    "response = client.embeddings.create(\n",
    "    model=\"Alibaba-NLP/gte-Qwen2-7B-instruct\",\n",
    "    input=\"How are you today\",\n",
    ")\n",
    "\n",
    "embedding = response.data[0].embedding[:10]\n",
    "print(embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlphaMeemory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
