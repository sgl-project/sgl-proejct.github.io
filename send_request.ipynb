{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch a server\n",
    "\n",
    "This code uses `subprocess.Popen` to start an SGLang server process, equivalent to executing \n",
    "\n",
    "```bash\n",
    "python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "--port 30000 --host 0.0.0.0 --log-level warning\n",
    "```\n",
    "in your command line and wait for the server to be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T00:31:32.892739Z",
     "iopub.status.busy": "2024-10-25T00:31:32.892570Z",
     "iopub.status.idle": "2024-10-25T00:32:12.114863Z",
     "shell.execute_reply": "2024-10-25T00:32:12.113974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is ready. Proceeding with the next steps.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "server_process = subprocess.Popen(\n",
    "    [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"sglang.launch_server\",\n",
    "        \"--model-path\",\n",
    "        \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        \"--port\",\n",
    "        \"30000\",\n",
    "        \"--host\",\n",
    "        \"0.0.0.0\",\n",
    "        \"--log-level\",\n",
    "        \"error\",\n",
    "    ],\n",
    "    text=True,\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            \"http://localhost:30000/v1/models\",\n",
    "            headers={\"Authorization\": \"Bearer None\"},\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            break\n",
    "    except requests.exceptions.RequestException:\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"Server is ready. Proceeding with the next steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a Request\n",
    "\n",
    "Once the server is running, you can send test requests using curl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T00:32:12.148930Z",
     "iopub.status.busy": "2024-10-25T00:32:12.148707Z",
     "iopub.status.idle": "2024-10-25T00:32:14.786517Z",
     "shell.execute_reply": "2024-10-25T00:32:14.785640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"f0a66df3040049a1acc9a9b8e9aa31ff\",\"object\":\"chat.completion\",\"created\":1729816334,\"model\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"LLM stands for Large Language Model. It's a type of artificial intelligence (AI) designed to process and understand human language. LLMs are trained on massive amounts of text data, which allows them to learn patterns, relationships, and context within language.\\n\\nLarge Language Models are typically characterized by their ability to:\\n\\n1. **Understand natural language:** LLMs can comprehend and interpret human language, including nuances, idioms, and context-dependent expressions.\\n2. **Generate human-like text:** LLMs can create text that is similar in style and structure to human-written text, making them useful for tasks like writing, translation, and summarization.\\n3. **Learn from data:** LLMs are trained on vast amounts of text data, which enables them to learn and improve their language understanding and generation capabilities.\\n\\nSome common applications of LLMs include:\\n\\n1. **Virtual assistants:** LLMs power virtual assistants like Siri, Alexa, and Google Assistant, enabling them to understand and respond to voice commands.\\n2. **Language translation:** LLMs can translate languages in real-time, making communication across languages more accessible.\\n3. **Text summarization:** LLMs can summarize long documents or articles into concise, easily digestible summaries.\\n4. **Chatbots:** LLMs are used in chatbots to provide customer support, answer frequently asked questions, and engage in conversations.\\n5. **Content generation:** LLMs can generate original content, such as articles, social media posts, and even entire books.\\n\\nLLMs have revolutionized the way we interact with language and have opened up new possibilities for language-based applications and services.\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":128009}],\"usage\":{\"prompt_tokens\":47,\"total_tokens\":380,\"completion_tokens\":333,\"prompt_tokens_details\":null}}"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:30000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer None\" \\\n",
    "  -d '{\"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is a LLM?\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Compatible API\n",
    "\n",
    "SGLang supports OpenAI-compatible APIs. Here are Python examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T00:32:14.789010Z",
     "iopub.status.busy": "2024-10-25T00:32:14.788414Z",
     "iopub.status.idle": "2024-10-25T00:32:15.538310Z",
     "shell.execute_reply": "2024-10-25T00:32:15.537297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='b3f805822e6d4eaa8c4359b80b3fc7c0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are 3 countries and their capitals:\\n\\n1. **Country:** Japan\\n**Capital:** Tokyo\\n\\n2. **Country:** Australia\\n**Capital:** Canberra\\n\\n3. **Country:** Brazil\\n**Capital:** Bras√≠lia', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), matched_stop=128009)], created=1729816335, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=46, prompt_tokens=49, total_tokens=95, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Always assign an api_key, even if not specified during server initialization.\n",
    "# Setting an API key during server initialization is strongly recommended.\n",
    "\n",
    "client = openai.Client(\n",
    "    base_url=\"http://127.0.0.1:30000/v1\", api_key=\"None\"\n",
    ")\n",
    "\n",
    "# Chat completion example\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T00:32:15.540666Z",
     "iopub.status.busy": "2024-10-25T00:32:15.540271Z",
     "iopub.status.idle": "2024-10-25T00:32:19.659985Z",
     "shell.execute_reply": "2024-10-25T00:32:19.659115Z"
    }
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def terminate_process(process):\n",
    "    try:\n",
    "        process.terminate()\n",
    "        try:\n",
    "            process.wait(timeout=5)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            if os.name != 'nt':\n",
    "                try:\n",
    "                    pgid = os.getpgid(process.pid)\n",
    "                    os.killpg(pgid, signal.SIGTERM)\n",
    "                    time.sleep(1)\n",
    "                    if process.poll() is None:\n",
    "                        os.killpg(pgid, signal.SIGKILL)\n",
    "                except ProcessLookupError:\n",
    "                    pass\n",
    "            else:\n",
    "                process.kill()\n",
    "            process.wait()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "    finally:\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "terminate_process(server_process)\n",
    "time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlphaMeemory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
