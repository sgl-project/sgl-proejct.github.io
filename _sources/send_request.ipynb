{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch a server\n",
    "\n",
    "This code uses `subprocess.Popen` to start an SGLang server process, equivalent to executing \n",
    "\n",
    "```bash\n",
    "python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "--port 30000 --host 0.0.0.0 --log-level warning\n",
    "```\n",
    "in your command line and wait for the server to be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T00:13:55.948354Z",
     "iopub.status.busy": "2024-10-25T00:13:55.948192Z",
     "iopub.status.idle": "2024-10-25T00:13:55.952228Z",
     "shell.execute_reply": "2024-10-25T00:13:55.951694Z"
    }
   },
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import time\n",
    "# import requests\n",
    "# import os\n",
    "\n",
    "# server_process = subprocess.Popen(\n",
    "#     [\n",
    "#         \"python\",\n",
    "#         \"-m\",\n",
    "#         \"sglang.launch_server\",\n",
    "#         \"--model-path\",\n",
    "#         \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "#         \"--port\",\n",
    "#         \"30000\",\n",
    "#         \"--host\",\n",
    "#         \"0.0.0.0\",\n",
    "#         \"--log-level\",\n",
    "#         \"error\",\n",
    "#     ],\n",
    "#     text=True,\n",
    "#     stdout=subprocess.DEVNULL,\n",
    "#     stderr=subprocess.DEVNULL,\n",
    "# )\n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         response = requests.get(\n",
    "#             \"http://localhost:30000/v1/models\",\n",
    "#             headers={\"Authorization\": \"Bearer None\"},\n",
    "#         )\n",
    "#         if response.status_code == 200:\n",
    "#             break\n",
    "#     except requests.exceptions.RequestException:\n",
    "#         time.sleep(1)\n",
    "\n",
    "# print(\"Server is ready. Proceeding with the next steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a Request\n",
    "\n",
    "Once the server is running, you can send test requests using curl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T00:13:55.953847Z",
     "iopub.status.busy": "2024-10-25T00:13:55.953672Z",
     "iopub.status.idle": "2024-10-25T00:13:55.956121Z",
     "shell.execute_reply": "2024-10-25T00:13:55.955644Z"
    }
   },
   "outputs": [],
   "source": [
    "# !curl http://localhost:30000/v1/chat/completions \\\n",
    "#   -H \"Content-Type: application/json\" \\\n",
    "#   -H \"Authorization: Bearer None\" \\\n",
    "#   -d '{\"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is a LLM?\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Compatible API\n",
    "\n",
    "SGLang supports OpenAI-compatible APIs. Here are Python examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T00:13:55.957673Z",
     "iopub.status.busy": "2024-10-25T00:13:55.957513Z",
     "iopub.status.idle": "2024-10-25T00:13:55.960191Z",
     "shell.execute_reply": "2024-10-25T00:13:55.959718Z"
    }
   },
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# # Always assign an api_key, even if not specified during server initialization.\n",
    "# # Setting an API key during server initialization is strongly recommended.\n",
    "\n",
    "# client = openai.Client(\n",
    "#     base_url=\"http://127.0.0.1:30000/v1\", api_key=\"None\"\n",
    "# )\n",
    "\n",
    "# # Chat completion example\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "#         {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "#     ],\n",
    "#     temperature=0,\n",
    "#     max_tokens=64,\n",
    "# )\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T00:13:55.961709Z",
     "iopub.status.busy": "2024-10-25T00:13:55.961548Z",
     "iopub.status.idle": "2024-10-25T00:13:55.964568Z",
     "shell.execute_reply": "2024-10-25T00:13:55.964091Z"
    }
   },
   "outputs": [],
   "source": [
    "# import signal  # Add this at the top with other imports\n",
    "# import gc  # Add this for garbage collection\n",
    "# import torch\n",
    "\n",
    "# def terminate_process(process):\n",
    "#     try:\n",
    "#         # Gracefully terminate the process\n",
    "#         process.terminate()\n",
    "#         try:\n",
    "#             process.wait(timeout=5)\n",
    "#         except subprocess.TimeoutExpired:\n",
    "#             if os.name != 'nt':  # Unix-like systems\n",
    "#                 try:\n",
    "#                     pgid = os.getpgid(process.pid)\n",
    "#                     os.killpg(pgid, signal.SIGTERM)\n",
    "#                     time.sleep(1)\n",
    "#                     if process.poll() is None:\n",
    "#                         os.killpg(pgid, signal.SIGKILL)\n",
    "#                 except ProcessLookupError:\n",
    "#                     pass\n",
    "#             else:  # Windows\n",
    "#                 process.kill()\n",
    "#             process.wait()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Warning: {e}\")\n",
    "#     finally:\n",
    "#         # Force cleanup\n",
    "#         gc.collect()\n",
    "#         if torch.cuda.is_available():  # Add this import if using PyTorch\n",
    "#             torch.cuda.empty_cache()\n",
    "#             torch.cuda.ipc_collect()\n",
    "\n",
    "# # When terminating the first server\n",
    "# terminate_process(server_process)\n",
    "# time.sleep(2)  # Give some time for cleanup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlphaMeemory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
